# LLMs Comparison - Proyecto de Navegaci칩n Aut칩noma en CARLA

Este proyecto implementa y eval칰a diferentes modelos de lenguaje multimodal (LMMs) para la tarea de navegaci칩n aut칩noma en intersecciones utilizando el simulador CARLA. El sistema combina im치genes BEV (Bird's Eye View) y frontales para predecir la direcci칩n correcta (Straight, Right, Left) que debe tomar un veh칤culo aut칩nomo.

---

## 游늬 Estructura del Proyecto

### `dataset/`

Contiene el dataset limpio tras la revisi칩n individualizada de cada muestra. La estructura es la siguiente:

- **`data.json`**: Archivo JSON principal que contiene todas las muestras del dataset. Para cada muestra se incluye:
  - Rutas a las 4 im치genes (frontal RGB, frontal segmentada, BEV RGB, BEV segmentada)
  - Coordenadas de origen y destino en p칤xeles
  - Ground truth (direcci칩n correcta)
  - Weather empleado (solo afecta a las im치genes sin segmentar)

- **`Simulation_scripts/dataset/front/`**: Almacena las im치genes frontales del veh칤culo (RGB y segmentadas)

- **`Simulation_scripts/dataset/bev/`**: Almacena las im치genes BEV (Bird's Eye View) del mapa

- **Im치genes especiales (`clean_bev_ss_image`)**: Mapas BEV segmentados vac칤os (sin indicadores de origen/destino). Estas im치genes sirvieron como base para posteriormente agregar los indicadores visuales de origen (flecha roja) y destino (cuadrado azul).

---

### `Simulation_scripts/`

Contiene los scripts para la generaci칩n del dataset utilizando el simulador CARLA.

#### `Simulation_scripts/gross_dataset/`

Directorio con todas las muestras generadas, a칰n sin revisar manualmente. Incluye:

- **`intersections.json`**: Contiene la misma informaci칩n que `data.json`, pero con informaci칩n de depuraci칩n extendida para facilitar el an치lisis y filtrado de muestras.

#### Scripts principales

- **`carla_agent_screenless.py`**: Script principal que controla el agente aut칩nomo en CARLA. Se encarga de:
  - Inicializar el veh칤culo ego y los sensores (c치maras RGB y de segmentaci칩n sem치ntica)
  - Capturar im치genes frontales y BEV durante la navegaci칩n
  - Detectar intersecciones y guardar autom치ticamente las muestras
  - Generar las anotaciones con las coordenadas de origen/destino y el ground truth

- **`mc_min_screenless.py`**: M칩dulo auxiliar que proporciona funciones para inicializar el entorno de CARLA y gestionar el bucle de juego sin interfaz gr치fica (modo headless).

- **`mc_utils_screenless.py`**: Utilidades adicionales para la gesti칩n del mundo de CARLA, incluyendo funciones para obtener el veh칤culo ego y cerrar correctamente la simulaci칩n.

- **`routes.json`**: Archivo de configuraci칩n que define las rutas de navegaci칩n. Las rutas se generan aleatoriamente para aumentar la variabilidad del dataset y cubrir diferentes escenarios de intersecciones.

---

### `Test_scripts/`

Contiene los scripts y recursos necesarios para evaluar los modelos LMM en el dataset de CARLA.

#### `Test_scripts/prompts/`

Directorio que almacena los 4 prompts necesarios para ejecutar las pruebas. Cada prompt se corresponde con una configuraci칩n de entrada espec칤fica:

- **Prompt BEV**: Solo imagen BEV segmentada
- **Prompt BEV + Frontal**: Imagen BEV segmentada + imagen frontal segmentada
- **Prompt BEV + Coords**: Imagen BEV segmentada + coordenadas de origen y destino
- **Prompt BEV + Frontal + Coords**: Imagen BEV segmentada + imagen frontal segmentada + coordenadas

#### Scripts principales

- **`test_carla.py`**: Script principal de evaluaci칩n que:
  - Carga el dataset y lo divide en conjuntos de entrenamiento y validaci칩n
  - Ejecuta las diferentes configuraciones de prueba sobre los modelos
  - Calcula m칠tricas de rendimiento (accuracy, F1-score, precision, recall, matriz de confusi칩n)
  - Guarda los resultados y las respuestas del modelo en formato JSON
  - Identifica y guarda las muestras mal clasificadas para an치lisis posterior

- **`models_api.py`**: M칩dulo que proporciona una API unificada para cargar y ejecutar diferentes modelos LMM:
  - Funciones `load_lmm()` para cargar modelos base y fine-tuned (LoRA/DoRA)
  - Funciones `call_lmm()` para realizar inferencia con soporte para In-Context Learning (ICL)
  - Soporte para m칰ltiples arquitecturas: LLaVA, Gemma, Qwen, InternVL, Gemini API
  - Gesti칩n autom치tica de distribuci칩n multi-GPU y cuantizaci칩n

---

### `CIL_adaptations/`

Contiene las adaptaciones del modelo CIL++ ([Conditional Imitation Learning++](https://arxiv.org/pdf/2302.03198)) para este proyecto.

#### Arquitectura de CIL++

CIL++ es una arquitectura de aprendizaje por imitaci칩n condicional dise침ada para conducci칩n aut칩noma. El modelo:

- Utiliza una red neuronal convolucional (CNN) como encoder visual para extraer caracter칤sticas de las im치genes
- Implementa un mecanismo de atenci칩n para fusionar informaci칩n de m칰ltiples vistas
- Predice acciones de control (steering, throttle, brake) condicionadas a comandos de alto nivel
- En este proyecto, se ha adaptado para predecir directamente la direcci칩n en intersecciones (clasificaci칩n)

#### Archivos

- **`CIL_singleview.py`**: Adaptaci칩n de CIL++ para la configuraci칩n donde el modelo **solo recibe im치genes BEV segmentadas**. Esta versi칩n simplificada procesa 칰nicamente la vista a칠rea para tomar decisiones.

- **`CIL_binaryview.py`**: Adaptaci칩n de CIL++ para la configuraci칩n donde el modelo recibe **tanto la imagen BEV como la imagen frontal**. 
  - Se puede configurar si se desean im치genes segmentadas o RGB modificando la variable `SEGMENTED_FRONT` al cargar el dataset
  - Implementa fusi칩n de caracter칤sticas de ambas vistas mediante concatenaci칩n o atenci칩n

**Nota**: Estos archivos requieren de toda la estructura del proyecto CIL++ para ejecutarse correctamente. Es necesario tener instaladas las dependencias espec칤ficas de CIL++ y la estructura de directorios completa del repositorio original.

---

### `Improvements/`

Contiene mejoras y experimentos adicionales sobre los modelos base.

#### `Improvements/PEFT/`

Directorio dedicado al fine-tuning de modelos mediante **DoRA (Weight-Decomposed Low-Rank Adaptation)** utilizando la librer칤a [PEFT de HuggingFace](https://github.com/huggingface/peft).

DoRA es una variante mejorada de LoRA que descompone los pesos en magnitud y direcci칩n, logrando mejor rendimiento con el mismo n칰mero de par치metros entrenables.

#### Flujo de trabajo para fine-tuning

1. **`create_dataset.py`**: Envuelve el dataset de CARLA en el formato de la API `Datasets` de HuggingFace, creando un objeto `Dataset` compatible con los pipelines de entrenamiento.

2. **`create_peft_wrapper.py`**: Prepara el modelo y los datos para el entrenamiento PEFT:
   - Tokeniza correctamente los prompts seg칰n el formato de chat de cada modelo
   - Procesa las im치genes con el processor correspondiente
   - Configura los par치metros de DoRA (rank, alpha, target modules)
   - Crea el wrapper PEFT sobre el modelo base

3. **`train_model.py`**: Ejecuta el entrenamiento de tipo SFT (Supervised Fine-Tuning):
   - Utiliza `SFTTrainer` de la librer칤a `trl` de HuggingFace
   - Implementa callbacks personalizados para logging y checkpointing
   - Guarda el modelo fine-tuned y los adaptadores LoRA/DoRA

#### Archivos adicionales

- **`get_linear_layers.py`**: Utilidad para identificar autom치ticamente las capas lineales del modelo que ser치n objetivo del fine-tuning con PEFT.

- **`trainer_callback.py`**: Define callbacks personalizados para el proceso de entrenamiento (e.g., logging de m칠tricas, guardado de checkpoints intermedios).

---

### `requirements.txt`

Archivo con todas las dependencias del proyecto. Para instalar las librer칤as necesarias, ejecuta:

```bash
pip install -r requirements.txt
```

**Nota**: Algunas dependencias espec칤ficas de CARLA pueden requerir instalaci칩n manual. Consulta la [documentaci칩n oficial de CARLA](https://carla.readthedocs.io/) para m치s detalles.

---

## 游 Uso R치pido

### 1. Generar dataset en CARLA

```bash
cd Simulation_scripts
python carla_agent_screenless.py --map Town01_Opt --weather ClearNoon
```

### 2. Evaluar un modelo

```bash
cd Simulation_scripts
python ../Test_scripts/test_carla.py --model "google/gemma-3-12b-it" --do-tests test_bev test_bev_frontal
```

### 3. Fine-tuning con PEFT

```bash
cd Improvements/PEFT
python create_dataset.py
python create_peft_wrapper.py
python train_model.py
```

---

## 游늵 Resultados

Los resultados de las evaluaciones se guardan en `Test_scripts/test_results/{model_name}/`:

- `results.json`: M칠tricas de rendimiento (accuracy, F1, precision, recall, confusion matrix)
- `answers.json`: Respuestas completas del modelo para cada muestra
- `wrong_classified/`: Im치genes de los casos mal clasificados para an치lisis

---

## 游닇 Citas

Si utilizas este c칩digo, por favor cita el paper de CIL++:

```bibtex
@article{cilplusplus2023,
  title={Conditional Imitation Learning++},
  author={...},
  journal={arXiv preprint arXiv:2302.03198},
  year={2023}
}
```

---

## 游닎 Contacto

Para preguntas o colaboraciones, contacta con el equipo de desarrollo del proyecto.
